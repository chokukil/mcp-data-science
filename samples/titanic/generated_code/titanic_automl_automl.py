"""
Generated Code for Operation: titanic_automl
Code Type: automl
Generated at: 2025-06-09T01:01:06.259523

This code was automatically generated by the Advanced Data Science MCP Server.
You can run this code independently in your Python environment.
"""


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from sklearn.svm import SVC, SVR
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import joblib

# Additional advanced models (if available)
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except:
    XGB_AVAILABLE = False

try:
    import lightgbm as lgb
    LGB_AVAILABLE = True
except:
    LGB_AVAILABLE = False

# Load dataset
df = pd.read_csv('dataset_titanic_load.csv')

# Data preprocessing
processed_df = df.copy()

# Handle missing values
for col in df.columns:
    if df[col].isnull().sum() > 0:
        if df[col].dtype.kind in 'biufc':  # numeric
            processed_df[col] = processed_df[col].fillna(processed_df[col].median())
        else:  # categorical
            processed_df[col] = processed_df[col].fillna(processed_df[col].mode()[0])

# Handle categorical variables
categorical_cols = processed_df.select_dtypes(include=['object', 'category']).columns
if 'survived' in categorical_cols:
    categorical_cols = categorical_cols.drop('survived')

for col in categorical_cols:
    if processed_df[col].nunique() > 10:
        # Frequency encoding for high cardinality
        freq_encoding = processed_df[col].value_counts().to_dict()
        processed_df[col] = processed_df[col].map(freq_encoding)
    else:
        # One-hot encoding for low cardinality
        dummies = pd.get_dummies(processed_df[col], prefix=col)
        processed_df = pd.concat([processed_df.drop(col, axis=1), dummies], axis=1)

# Prepare features and target
X = processed_df.drop('survived', axis=1)
y = processed_df['survived']

# Handle target encoding for classification
label_encoder = None
if 'classification' == 'classification' and y.dtype == 'object':
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42,
    stratify=y if 'classification' == 'classification' else None
)

# Model selection
models = {}
if 'classification' == 'classification':
    models.update({
        'Random Forest': RandomForestClassifier(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42),
        'SVM': SVC(random_state=42)
    })
    if XGB_AVAILABLE:
        models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
    if LGB_AVAILABLE:
        models['LightGBM'] = lgb.LGBMClassifier(random_state=42, verbose=-1)
else:  # regression
    models.update({
        'Random Forest': RandomForestRegressor(random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(random_state=42),
        'Linear Regression': LinearRegression(),
        'Ridge': Ridge(random_state=42),
        'SVR': SVR()
    })
    if XGB_AVAILABLE:
        models['XGBoost'] = xgb.XGBRegressor(random_state=42)
    if LGB_AVAILABLE:
        models['LightGBM'] = lgb.LGBMRegressor(random_state=42, verbose=-1)

# Train and evaluate models
model_scores = {}
best_score = 0
best_model = None
best_model_name = None

for name, model in models.items():
    print(f"Training {name}...")
    
    # Cross-validation
    scoring = 'accuracy' if 'classification' == 'classification' else 'neg_mean_squared_error'
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scoring)
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    
    if 'classification' == 'classification':
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        model_scores[name] = {
            'cv_score': cv_scores.mean(),
            'test_accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
        print(f"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
        
        if accuracy > best_score:
            best_score = accuracy
            best_model = model
            best_model_name = name
    else:  # regression
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        
        model_scores[name] = {
            'cv_score': -cv_scores.mean(),
            'test_r2': r2,
            'rmse': rmse,
            'mae': mae
        }
        print(f"{name} - R2: {r2:.4f}, RMSE: {rmse:.4f}")
        
        if r2 > best_score:
            best_score = r2
            best_model = model
            best_model_name = name

print(f"\nBest model: {best_model_name}")
print(f"Best model scores: {model_scores[best_model_name]}")

# Save the best model
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
if label_encoder:
    joblib.dump(label_encoder, 'label_encoder.pkl')

print("\nModels saved successfully!")

# Feature importance (if available)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 Feature Importances:")
    print(feature_importance.head(10))
    
    # Plot feature importance
    plt.figure(figsize=(10, 6))
    top_features = feature_importance.head(10)
    plt.barh(top_features['feature'], top_features['importance'])
    plt.xlabel('Importance')
    plt.title(f'Top 10 Feature Importances - {best_model_name}')
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
