# íƒ€ì´íƒ€ë‹‰ ìƒì¡´ ì˜ˆì¸¡ AutoML ë¶„ì„ ë³´ê³ ì„œ

**ë°ì´í„°ì…‹**: Titanic Dataset  
**ë¶„ì„ ì¼ì‹œ**: 2025ë…„ 06ì›” 09ì¼ 01ì‹œ 01ë¶„  
**ë³´ê³ ì„œ ID**: `report_104896aa`

---

## ğŸ“‹ Executive Summary

ë³¸ ë¶„ì„ì—ì„œëŠ” **Titanic Dataset** ë°ì´í„°ì…‹ì— ëŒ€í•œ í¬ê´„ì ì¸ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. 
ìµœì¢…ì ìœ¼ë¡œ **Extra Trees** ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, í…ŒìŠ¤íŠ¸ ì •í™•ë„ **94.3%**, F1 ì ìˆ˜ **0.942**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

---

## ëª©ì°¨

1. [ë°ì´í„°ì…‹ ê°œìš”](#1-ë°ì´í„°ì…‹-ê°œìš”)
2. [íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)](#2-íƒìƒ‰ì -ë°ì´í„°-ë¶„ì„-eda)
3. [ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§](#3-ë¨¸ì‹ ëŸ¬ë‹-ëª¨ë¸ë§)
4. [ì‹œê°í™” ê²°ê³¼](#4-ì‹œê°í™”-ê²°ê³¼)
5. [ìƒì„±ëœ ì½”ë“œ](#5-ìƒì„±ëœ-ì½”ë“œ)
6. [ì‚°ì¶œë¬¼ í™œìš© ê°€ì´ë“œ](#6-ì‚°ì¶œë¬¼-í™œìš©-ê°€ì´ë“œ)
7. [ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­](#7-ê²°ë¡ -ë°-ê¶Œì¥ì‚¬í•­)

---

## 1. ë°ì´í„°ì…‹ ê°œìš”

### 1.1 ë°ì´í„° ê¸°ë³¸ ì •ë³´

- **ì›ë³¸ íŒŒì¼**: `C:\langfuse\cherry\sandbox\datasets\titanic.csv`
- **íŒŒì¼ í¬ê¸°**: 0.10 MB
- **ë°ì´í„° ê·œëª¨**: 1,310 í–‰ Ã— 14 ì—´
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 0.52 MB

### 1.2 ë³€ìˆ˜ êµ¬ì„±

**ìˆ˜ì¹˜í˜• ë³€ìˆ˜ (7ê°œ)**: `pclass, survived, age, sibsp, parch, fare, body`

**ë²”ì£¼í˜• ë³€ìˆ˜ (7ê°œ)**: `name, sex, ticket, cabin, embarked, boat, home.dest`

### 1.3 ë°ì´í„° í’ˆì§ˆ

- **ì „ì²´ ê²°ì¸¡ê°’**: 3,869ê°œ (21.1%)
- **ì£¼ìš” ê²°ì¸¡ ë³€ìˆ˜**:

  - `body`: 1,189ê°œ (90.8%)
  - `cabin`: 1,015ê°œ (77.5%)
  - `boat`: 824ê°œ (62.9%)
  - `home.dest`: 565ê°œ (43.1%)
  - `age`: 264ê°œ (20.2%)

### 1.4 ë°ì´í„° íŒŒì¼

| íŒŒì¼ëª… | í¬ê¸° (MB) | ì„¤ëª… |
|--------|-----------|------|
| `dataset_titanic_load.csv` | 0.11 | ì²˜ë¦¬ëœ ë°ì´í„° |
| `titanic.csv` | 0.10 | ì›ë³¸ ë°ì´í„° |

## 2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)

### 2.1 ê¸°ìˆ í†µê³„ ìš”ì•½

#### ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„

| ë³€ìˆ˜ | í‰ê·  | í‘œì¤€í¸ì°¨ | ìµœì†Ÿê°’ | ì¤‘ì•™ê°’ | ìµœëŒ“ê°’ | ê²°ì¸¡ê°’ |
|------|------|----------|--------|--------|--------|--------|
| `pclass` | 2.28 | 0.84 | 1.00 | 3.00 | 3.00 | 1 |
| `survived` | 0.38 | 0.49 | 0.00 | 0.00 | 1.00 | 1 |
| `age` | 29.89 | 14.30 | 0.17 | 28.00 | 76.00 | 264 |
| `sibsp` | 0.50 | 1.04 | 0.00 | 0.00 | 8.00 | 1 |
| `parch` | 0.38 | 0.87 | 0.00 | 0.00 | 9.00 | 1 |
| `fare` | 33.89 | 51.73 | 0.00 | 14.50 | 512.33 | 2 |
| `body` | 156.56 | 95.72 | 7.00 | 147.00 | 328.00 | 1,189 |

### 2.3 ì£¼ìš” ë°œê²¬ì‚¬í•­

- âœ… Analysis performed on complete dataset
- âš ï¸ High missing data ratio: 21.1% - advanced imputation needed
- ğŸ“Š High outlier columns: parch, fare - consider robust methods
- ğŸ·ï¸ High cardinality columns: name, ticket, cabin - need advanced encoding

## 3. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§

### 3.1 ë¬¸ì œ ì •ì˜ ë° ì ‘ê·¼ë²•

- **ë¬¸ì œ ìœ í˜•**: Classification
- **íƒ€ê²Ÿ ë³€ìˆ˜**: `survived`

### 3.2 ë°ì´í„° ì „ì²˜ë¦¬

**ì ìš©ëœ ì „ì²˜ë¦¬ ê¸°ë²•**:

**ê²°ì¸¡ê°’ ì²˜ë¦¬**:

- pclass: filled with median
- survived: filled with median
- name: filled with mode
- sex: filled with mode
- age: filled with median
- ... ë° 9ê°œ ì¶”ê°€ ë³€ìˆ˜

**ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©**:

- name: frequency encoding
- sex: one-hot encoding
- ticket: frequency encoding
- cabin: frequency encoding
- embarked: one-hot encoding
- boat: frequency encoding
- home.dest: frequency encoding

### 3.3 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | êµì°¨ê²€ì¦ ì ìˆ˜ | í…ŒìŠ¤íŠ¸ ì •í™•ë„ | Precision | Recall | F1 Score |
|------|---------------|---------------|-----------|--------|----------|
| Random Forest | 0.9437 | 0.9351 | 0.9362 | 0.9351 | 0.9345 |
| Gradient Boosting | 0.9446 | 0.9389 | 0.9416 | 0.9389 | 0.9381 |
| Extra Trees | 0.9360 | 0.9427 | 0.9429 | 0.9427 | 0.9424 |
| Logistic Regression | 0.9503 | 0.9237 | 0.9258 | 0.9237 | 0.9226 |
| SVM | 0.9484 | 0.9275 | 0.9303 | 0.9275 | 0.9264 |
| K-Nearest Neighbors | 0.9275 | 0.9084 | 0.9082 | 0.9084 | 0.9078 |
| Naive Bayes | 0.4885 | 0.4962 | 0.7633 | 0.4962 | 0.4266 |
| Decision Tree | 0.9189 | 0.9160 | 0.9158 | 0.9160 | 0.9159 |
| MLP | 0.9322 | 0.9198 | 0.9206 | 0.9198 | 0.9190 |
| Gaussian Process | 0.9332 | 0.9237 | 0.9237 | 0.9237 | 0.9232 |
| QDA | 0.5763 | 0.5344 | 0.7178 | 0.5344 | 0.4973 |
| XGBoost | 0.9408 | 0.9389 | 0.9398 | 0.9389 | 0.9384 |
| LightGBM | 0.9408 | 0.9389 | 0.9405 | 0.9389 | 0.9382 |
| CatBoost | 0.9465 | 0.9313 | 0.9337 | 0.9313 | 0.9303 |

### 3.4 ìµœì  ëª¨ë¸: Extra Trees

**Extra Trees** ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì£¼ìš” ì„±ëŠ¥ ì§€í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: 94.3%
- **F1 Score**: 0.9424
- **êµì°¨ê²€ì¦ ì ìˆ˜**: 0.9360 Â± 0.0207

### 3.5 íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„

**ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±**:

| ìˆœìœ„ | íŠ¹ì„± | ì¤‘ìš”ë„ | ê¸°ì—¬ë„ |
|------|------|--------|--------|
| 1 | `boat` | 0.5867 | 58.7% |
| 2 | `sex_male` | 0.0911 | 9.1% |
| 3 | `sex_female` | 0.0840 | 8.4% |
| 4 | `fare` | 0.0598 | 6.0% |
| 5 | `age` | 0.0561 | 5.6% |
| 6 | `cabin` | 0.0239 | 2.4% |
| 7 | `pclass` | 0.0228 | 2.3% |
| 8 | `home.dest` | 0.0188 | 1.9% |
| 9 | `ticket` | 0.0138 | 1.4% |
| 10 | `sibsp` | 0.0114 | 1.1% |

## 4. ì‹œê°í™” ê²°ê³¼

### 4.1 ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ

![ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ 1](../plots/titanic_eda_advanced_dashboard.png)

í¬ê´„ì ì¸ ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œë¡œ ë°ì´í„°ì˜ ì „ë°˜ì ì¸ íŠ¹ì„±, ê²°ì¸¡ê°’ íŒ¨í„´, ìƒê´€ê´€ê³„ ë“±ì„ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4.2 ì°¨ì› ì¶•ì†Œ ë¶„ì„

![ì°¨ì› ì¶•ì†Œ ì‹œê°í™” 1](../plots/titanic_eda_dimensionality_reduction.png)

PCA, t-SNE, UMAP ë“±ì˜ ê³ ê¸‰ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŒ¨í„´ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 4.3 íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”

![íŠ¹ì„± ì¤‘ìš”ë„ 1](../plots/titanic_automl_feature_importance_20250609_010145.png)

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê° íŠ¹ì„±ì˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•œ ì°¨íŠ¸ì…ë‹ˆë‹¤. ìƒìœ„ íŠ¹ì„±ë“¤ì´ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.

ì´ 3ê°œì˜ ì‹œê°í™”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì°¨íŠ¸ëŠ” ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ë³´ì—¬ì£¼ë©°, ì¢…í•©ì ì¸ ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤.

## 5. ìƒì„±ëœ ì½”ë“œ

### 5.1 ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§ ì „ëµì„ í¬í•¨í•œ ë°ì´í„° ì¤€ë¹„ ì½”ë“œì…ë‹ˆë‹¤.

**íŒŒì¼ëª…**: `titanic_load_data_loading.py`  
**ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬**: `pandas, numpy`

**í•µì‹¬ ê¸°ëŠ¥**:

- ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ëŠ¥í˜• ìƒ˜í”Œë§
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©

### 5.2 íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì½”ë“œë¡œ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„, ìƒê´€ê´€ê³„, ë¶„í¬, ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

**íŒŒì¼ëª…**: `titanic_eda_eda.py`  
**ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬**: `pandas, seaborn, matplotlib, warnings, numpy`

**í•µì‹¬ ê¸°ëŠ¥**:

- ê¸°ìˆ í†µê³„ ê³„ì‚° (df.describe())
- ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™”
- ë³€ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
- ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ë° ì‹œê°í™”

**ì£¼ìš” êµ¬í˜„ ë‚´ìš©**:

- `import matplotlib.pyplot as plt`
- `print(df.describe())`
- `correlation_matrix = df[numeric_cols].corr()`
- `sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)`
- `plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')`
- `df[numeric_cols].hist(bins=30, alpha=0.7)`
- `plt.savefig('distributions.png', dpi=300, bbox_inches='tight')`
- `missing_values[missing_values > 0].plot(kind='bar')`
- `plt.savefig('missing_values.png', dpi=300, bbox_inches='tight')`

**ìƒì„±ë˜ëŠ” ì‚°ì¶œë¬¼**:

- ì‹œê°í™” íŒŒì¼: correlation_heatmap.png
- ì‹œê°í™” íŒŒì¼: distributions.png
- ì‹œê°í™” íŒŒì¼: missing_values.png

### 5.3 ìë™í™”ëœ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í•™ìŠµ, í‰ê°€ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒì¼ëª…**: `titanic_automl_automl.py`  
**ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬**: `pandas, lightgbm, sklearn, xgboost, joblib, numpy`

**í•µì‹¬ ê¸°ëŠ¥**:

- ê²°ì¸¡ê°’ ìë™ ì²˜ë¦¬ (ì¤‘ì•™ê°’/ìµœë¹ˆê°’)
- ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (ì›-í•«/ë¹ˆë„ ì¸ì½”ë”©)
- íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (StandardScaler)
- ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ ë° ì„±ëŠ¥ í‰ê°€
- ìµœì  ëª¨ë¸ ìë™ ì„ íƒ ë° ì €ì¥

**ì£¼ìš” êµ¬í˜„ ë‚´ìš©**:

- `from sklearn.model_selection import train_test_split, cross_val_score`
- `from sklearn.preprocessing import StandardScaler, LabelEncoder`
- `processed_df[col] = processed_df[col].fillna(processed_df[col].median())`
- `processed_df[col] = processed_df[col].fillna(processed_df[col].mode()[0])`
- `freq_encoding = processed_df[col].value_counts().to_dict()`
- `dummies = pd.get_dummies(processed_df[col], prefix=col)`
- `y = label_encoder.fit_transform(y)`
- `scaler = StandardScaler()`
- `X_scaled = scaler.fit_transform(X)`
- `X_train, X_test, y_train, y_test = train_test_split(`

**ìƒì„±ë˜ëŠ” ì‚°ì¶œë¬¼**:

- ëª¨ë¸ íŒŒì¼: best_model.pkl
- ëª¨ë¸ íŒŒì¼: scaler.pkl
- ëª¨ë¸ íŒŒì¼: label_encoder.pkl
- ì‹œê°í™” íŒŒì¼: feature_importance.png

## 6. ì‚°ì¶œë¬¼ í™œìš© ê°€ì´ë“œ

> âš ï¸ **ì¤‘ìš” ì•ˆë‚´ì‚¬í•­**
> 
> ì´ì „ê¹Œì§€ì˜ ê²€ì¦ëœ ì½”ë“œì™€ëŠ” ë‹¬ë¦¬, ì´ ì„¹ì…˜ì˜ ëª¨ë“  ì½”ë“œ ì˜ˆì œëŠ” **AIê°€ ìë™ìœ¼ë¡œ ìƒì„±**í•œ ê²ƒì…ë‹ˆë‹¤.
> 
> - âœ… **ì‹¤í–‰ ì „ ë°˜ë“œì‹œ ì½”ë“œë¥¼ ê²€í† **í•˜ê³  í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
> - âœ… **íŒŒì¼ ê²½ë¡œì™€ ë³€ìˆ˜ëª…**ì´ ì‹¤ì œ í™˜ê²½ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
> - âœ… **í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜**ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
> - âœ… **ë°ì´í„° í˜•ì‹ê³¼ êµ¬ì¡°**ê°€ ì˜ˆì œì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”
> - âœ… **ì—ëŸ¬ ì²˜ë¦¬ ì½”ë“œë¥¼ ì¶”ê°€**í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì´ì„¸ìš”
> 
> ìë™ ìƒì„±ëœ ì½”ë“œëŠ” **ì°¸ê³ ìš© í…œí”Œë¦¿**ìœ¼ë¡œ í™œìš©í•˜ì‹œê³ , ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

---

### 6.1 ë¶„ë¥˜ ëª¨ë¸ í™œìš©

#### ê¸°ë³¸ ì˜ˆì¸¡ ìˆ˜í–‰
```python
import pandas as pd
import joblib

# ëª¨ë¸ ë¡œë“œ
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')
label_encoder = joblib.load('label_encoder.pkl')  # ë²”ì£¼í˜• íƒ€ê²Ÿì¸ ê²½ìš°

# ìƒˆ ë°ì´í„° ì¤€ë¹„
new_data = pd.DataFrame({
    'feature1': [1.5, 2.3],
    'feature2': [0.8, 1.2],
})

# ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
new_data_scaled = scaler.transform(new_data)
predictions = model.predict(new_data_scaled)
probabilities = model.predict_proba(new_data_scaled)

# ë¼ë²¨ ë””ì½”ë”© (í•„ìš”í•œ ê²½ìš°)
if label_encoder:
    predictions = label_encoder.inverse_transform(predictions)

# ê²°ê³¼ ì¶œë ¥
for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
    confidence = prob.max()
    print(f'ìƒ˜í”Œ {i+1}: ì˜ˆì¸¡={pred}, ì‹ ë¢°ë„={confidence:.1%}')
```

### 6.2 ëª¨ë¸ API ì„œë¹™

FastAPIë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì„œë¹™ ì˜ˆì œ:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import pandas as pd
import numpy as np

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ì‹œ í•œë²ˆë§Œ)
model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

class PredictionRequest(BaseModel):
    features: dict

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float = None

@app.post('/predict', response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    try:
        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
        df = pd.DataFrame([request.features])
        
        # ì „ì²˜ë¦¬
        df_scaled = scaler.transform(df)
        
        # ì˜ˆì¸¡
        prediction = model.predict(df_scaled)[0]
        
        # ì‹ ë¢°ë„ (ê°€ëŠ¥í•œ ê²½ìš°)
        confidence = None
        if hasattr(model, 'predict_proba'):
            proba = model.predict_proba(df_scaled)[0]
            confidence = float(proba.max())
        
        return PredictionResponse(
            prediction=float(prediction),
            confidence=confidence
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# ì‹¤í–‰: uvicorn main:app --reload
```

### 6.3 ì „ì²´ ë¶„ì„ ì¬í˜„

ìƒì„±ëœ ì½”ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ë¶„ì„ì„ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# 1. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install pandas numpy matplotlib seaborn scikit-learn joblib

pip install xgboost  # ê³ ê¸‰ ë¶€ìŠ¤íŒ… ëª¨ë¸
pip install lightgbm  # ê³ ê¸‰ ë¶€ìŠ¤íŒ… ëª¨ë¸

# 2. ì½”ë“œ ì‹¤í–‰ (ìˆœì„œëŒ€ë¡œ)
python load_*.py      # ë°ì´í„° ë¡œë”©
python eda_*.py       # íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
python automl_*.py    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§

# 3. ê²°ê³¼ í™•ì¸
ls *.png  # ìƒì„±ëœ ì‹œê°í™”
ls *.pkl  # ì €ì¥ëœ ëª¨ë¸
ls *.csv  # ì²˜ë¦¬ëœ ë°ì´í„°
```

### 6.4 ëª¨ë¸ ëª¨ë‹ˆí„°ë§

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•:

```python
import pandas as pd
import numpy as np
from datetime import datetime
import joblib

class ModelMonitor:
    def __init__(self, model_path, threshold=0.1):
        self.model = joblib.load(model_path)
        self.threshold = threshold
        self.predictions_log = []
        
    def predict_and_log(self, X):
        # ì˜ˆì¸¡
        prediction = self.model.predict(X)
        
        # ë¡œê¹…
        self.predictions_log.append({
            'timestamp': datetime.now(),
            'input_shape': X.shape,
            'prediction': prediction
        })
        
        return prediction
    
    def check_drift(self, recent_performance):
        # ì„±ëŠ¥ ì €í•˜ ê°ì§€
        if recent_performance < self.baseline_performance - self.threshold:
            print('âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€! ëª¨ë¸ ì¬í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
            return True
        return False
```

## 7. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### ì£¼ìš” ê²°ë¡ 

- **Extra Trees** ëª¨ë¸ì´ 94.3%ì˜ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- ë°ì´í„° í’ˆì§ˆ ì´ìŠˆê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤ (ê²°ì¸¡ë¥  21.1%).
- íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ì„ í†µí•´ í•µì‹¬ ì˜ˆì¸¡ ë³€ìˆ˜ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

### ê¶Œì¥ì‚¬í•­

- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì´ ê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê³ ê¸‰ ê²°ì¸¡ê°’ ëŒ€ì²´ ê¸°ë²•(MICE, KNN ë“±)ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.
- ì¤‘ìš”ë„ê°€ ë‚®ì€ íŠ¹ì„±ì„ ì œê±°í•˜ì—¬ ëª¨ë¸ì„ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì •ê¸°ì ì¸ ëª¨ë¸ ì¬í•™ìŠµì„ í†µí•´ ë°ì´í„° ë“œë¦¬í”„íŠ¸ì— ëŒ€ì‘í•˜ì„¸ìš”.
- ëª¨ë¸ ë°°í¬ ì „ A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ê²€ì¦í•˜ì„¸ìš”.
- ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¡°ê¸°ì— ê°ì§€í•˜ì„¸ìš”.

---

**ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ**: 2025-06-09 01:01:52  
**ë¶„ì„ ë„êµ¬**: Advanced Data Science MCP Server  
**ë³´ê³ ì„œ ë²„ì „**: Enhanced v3.0  
**ë³´ê³ ì„œ ID**: `report_104896aa`
